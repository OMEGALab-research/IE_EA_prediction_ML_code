{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('outputs/non_constant_columns.pkl', 'rb') as f:\n",
    "    non_constant_columns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_energy_levels_df = pd.read_csv('outputs/processed_source_dataset.csv')\n",
    "source_energy_levels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_pickle('outputs/source_descriptors_processed.pkl')\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.concat((source_df, source_energy_levels_df[['HOMO_DFT']]), axis=1)\n",
    "del source_energy_levels_df\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = source_df[non_constant_columns]\n",
    "y = source_df['HOMO_DFT']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.read_pickle('outputs/target_descriptors_calculated_n_processed.pkl')\n",
    "target_df = target_df[target_df['HOMO_UPS'].notna()]\n",
    "target_df = target_df[target_df['Type'] != 'External Validation']\n",
    "len(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dev_set = 10\n",
    "dev_set_size = len(target_df)\n",
    "\n",
    "n_test_set = 5\n",
    "test_set_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_devs = []\n",
    "y_devs = []\n",
    "for i in range(n_dev_set):\n",
    "    X_dev = X_train[i*dev_set_size:i*dev_set_size+dev_set_size]\n",
    "    y_dev = y_train[i*dev_set_size:i*dev_set_size+dev_set_size]\n",
    "    X_devs.append(X_dev)\n",
    "    y_devs.append(y_dev)\n",
    "    print(f'Dev set {i}, size {X_dev.shape}, label size {y_dev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tests = []\n",
    "y_tests = []\n",
    "for i in range(n_test_set):\n",
    "    X_test = X_val[i*test_set_size:i*test_set_size+test_set_size]\n",
    "    y_test = y_val[i*test_set_size:i*test_set_size+test_set_size]\n",
    "    X_tests.append(X_test)\n",
    "    y_tests.append(y_test)\n",
    "    print(f'Test set {i}, size {X_test.shape}, label size {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(model, X_tests, y_tests, feature_indices=None):\n",
    "    maes = []\n",
    "    for i in range(5):\n",
    "        if feature_indices is None:\n",
    "            preds = model.predict(X_tests[i])\n",
    "            mae = mean_absolute_error(y_tests[i], preds)\n",
    "            maes.append(mae)\n",
    "        else:\n",
    "            preds = model.predict(X_tests[i][X_tests[i].columns[feature_indices]])\n",
    "            mae = mean_absolute_error(y_tests[i], preds)\n",
    "            maes.append(mae)\n",
    "\n",
    "    return np.round(np.min(maes), 4), np.round(np.mean(maes), 4), np.round(np.max(maes), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_results = []\n",
    "test_results = []\n",
    "diff = []\n",
    "for i in range(n_dev_set):\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    \n",
    "    cv_results = cross_validate(rf, X_devs[i], y_devs[i], scoring='neg_mean_absolute_error', return_estimator=True, n_jobs=-1)\n",
    "    dev_results.append(np.mean(cv_results['test_score'])*-1)\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    rf.fit(X_devs[i], y_devs[i])\n",
    "\n",
    "    min_mae, mean_mae, max_mae = calculate_mae(rf, X_tests, y_tests)\n",
    "    \n",
    "    test_results.append((min_mae, mean_mae, max_mae))\n",
    "\n",
    "    diff.append(abs(mean_mae - np.mean(cv_results['test_score'])*-1))\n",
    "    \n",
    "df = pd.DataFrame({'Dev': dev_results, 'Test': test_results, 'Diff': diff})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average difference between 5-fold CV error and test error is {np.mean(diff):.3f} eV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "results = []\n",
    "for k in [3, 5, 10]:\n",
    "    for n_rep in [3, 5, 10]:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=n_rep, random_state=0)\n",
    "        dev_results = []\n",
    "        test_results = []\n",
    "        diff = []\n",
    "        for i in range(n_dev_set):\n",
    "            rf = RandomForestRegressor(random_state=0)\n",
    "            cv_results = cross_validate(rf, X_devs[i], y_devs[i], scoring='neg_mean_absolute_error', return_estimator=True, cv=cv, n_jobs=-1)\n",
    "            dev_results.append(np.mean(cv_results['test_score'])*-1)\n",
    "            \n",
    "            rf = RandomForestRegressor(random_state=0)\n",
    "            rf.fit(X_devs[i], y_devs[i])\n",
    "\n",
    "            min_mae, mean_mae, max_mae = calculate_mae(rf, X_tests, y_tests)\n",
    "            \n",
    "            test_results.append((min_mae, mean_mae, max_mae))\n",
    "\n",
    "            diff.append(abs(mean_mae - np.mean(cv_results['test_score'])*-1))\n",
    "            \n",
    "        results.append((np.mean(diff), k, n_rep))\n",
    "        print(np.mean(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from collections import defaultdict\n",
    "dev_results = defaultdict(list)\n",
    "test_results = defaultdict(list)\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=0)\n",
    "\n",
    "for i in range(n_dev_set):\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    selector = RFE(rf, n_features_to_select=1, step=1)\n",
    "    selector = selector.fit(X_devs[i], y_devs[i])\n",
    "    sorted_feature_indices = np.argsort(selector.ranking_)\n",
    "    print(f'Feature selection {i} completed.')\n",
    "\n",
    "    for n_features_to_select in range(1, dev_set_size+1):\n",
    "        selected_feature_indices = sorted_feature_indices[:n_features_to_select]\n",
    "        \n",
    "        rf = RandomForestRegressor(random_state=0)\n",
    "        cv_results = cross_validate(\n",
    "            rf,\n",
    "            X_devs[i][X_devs[i].columns[selected_feature_indices]], y_devs[i],\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            return_estimator=True,\n",
    "            cv=cv,\n",
    "            n_jobs=-1)\n",
    "        dev_mae = np.mean(cv_results['test_score'])*-1\n",
    "\n",
    "        rf = RandomForestRegressor(random_state=0)\n",
    "        rf.fit(X_devs[i][X_devs[i].columns[selected_feature_indices]], y_devs[i])\n",
    "        _, test_mae, _ = calculate_mae(rf, X_tests, y_tests, selected_feature_indices)\n",
    "\n",
    "        print(i, n_features_to_select, dev_mae, test_mae)\n",
    "        \n",
    "        dev_results[i].append(dev_mae)\n",
    "        test_results[i].append(test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is a dev set\n",
    "# Each columns are number of features selected, first column being a single feature\n",
    "# Values are test set errors\n",
    "test_results_df = pd.DataFrame(test_results).T\n",
    "test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.plot(np.arange(1, dev_set_size+1), np.mean(test_results_df, axis=0))\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Average Test MAE (eV)')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='both', alpha=0.1)\n",
    "plt.xlim(0.5, dev_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.mean(test_results_df, axis=0)).sort_values(by=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_number_of_features = np.argmin(np.mean(test_results_df, axis=0)) + 1\n",
    "optimum_number_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum error\n",
    "np.mean(test_results_df[optimum_number_of_features-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=0)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 700),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 25),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_float('max_features', 0.3, 1.0)\n",
    "        }\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=0, **params)\n",
    "    cv_results = cross_validate(\n",
    "        rf,\n",
    "        X_selected, y,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        return_estimator=True,\n",
    "        cv=cv,\n",
    "        n_jobs=-1)\n",
    "    return np.mean(cv_results['test_score'])*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_errors = []\n",
    "untuned_test_error = []\n",
    "all_cv_errors = []\n",
    "for i in range(10):\n",
    "    cv_errors = []\n",
    "    print(f'Tuning dev set {i}')\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    selector = RFE(rf, n_features_to_select=optimum_number_of_features, step=1)\n",
    "    selector = selector.fit(X_devs[i], y_devs[i])\n",
    "    X_selected = selector.transform(X_devs[i])\n",
    "    y = y_devs[i]\n",
    "    print(X_selected.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # Untuned CV error\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    cv_results = cross_validate(\n",
    "        rf,\n",
    "        X_selected, y,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        return_estimator=True,\n",
    "        cv=cv,\n",
    "        n_jobs=-1)\n",
    "    cv_errors.append(np.mean(cv_results['test_score'])*-1)\n",
    "\n",
    "    # Untuned test error\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    rf.fit(X_selected, y)\n",
    "    # Calculate the corresponding test error\n",
    "    maes = []\n",
    "    for test_set_id in range(len(X_tests)):\n",
    "        X_test_selected = selector.transform(X_tests[test_set_id])\n",
    "        preds = rf.predict(X_test_selected)\n",
    "        mae = mean_absolute_error(y_tests[test_set_id], preds)\n",
    "        maes.append(mae)\n",
    "    untuned_test_error.append(np.mean(maes))\n",
    "\n",
    "    # Tuning\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=0))\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # Only cv error is used/calculated during tuning\n",
    "    # Now we will calculate the corresponding test error for each trial\n",
    "    lowest_value_so_far = None\n",
    "    best_params_so_far = None\n",
    "    best_params = []\n",
    "    test_errors = []\n",
    "    for trial in study.get_trials():\n",
    "        cv_errors.append(trial.value)\n",
    "        if lowest_value_so_far is None or trial.value < lowest_value_so_far:\n",
    "            best_params_so_far = trial.params\n",
    "            lowest_value_so_far = trial.value\n",
    "            # Train rf with best params so far\n",
    "            rf = RandomForestRegressor(random_state=0, **best_params_so_far)\n",
    "            rf.fit(X_selected, y)\n",
    "            # Calculate the corresponding test error\n",
    "            maes = []\n",
    "            for test_set_id in range(len(X_tests)):\n",
    "                X_test_selected = selector.transform(X_tests[test_set_id])\n",
    "                preds = rf.predict(X_test_selected)\n",
    "                mae = mean_absolute_error(y_tests[test_set_id], preds)\n",
    "                maes.append(mae)\n",
    "            test_error = np.mean(maes)\n",
    "        best_params.append(best_params_so_far)\n",
    "        test_errors.append(test_error)\n",
    "    all_test_errors.append(test_errors)\n",
    "    all_cv_errors.append(cv_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_cv_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = pd.DataFrame(all_test_errors)\n",
    "tuning_results.insert(0, \"Untuned\", untuned_test_error)\n",
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = list(np.arange(0, 50+1, 5))\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(list(np.mean(tuning_results, axis=0)), label='Test set error')\n",
    "plt.plot(list(np.mean(all_cv_errors, axis=0)), label='CV error')\n",
    "plt.xlabel('Number of trials')\n",
    "plt.ylabel('Average Test MAE (eV)')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='both', alpha=0.1)\n",
    "\n",
    "plt.xticks([0] + x_ticks[1:], ['Untuned'] + x_ticks[1:])\n",
    "plt.xlim(-0.5, 50.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to add 1 here\n",
    "# Because index 0 is untuned\n",
    "optimum_number_of_trials = np.argmin(np.mean(tuning_results, axis=0))\n",
    "optimum_number_of_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = list(np.arange(0, 50+1, 5))\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(list(np.mean(tuning_results, axis=0)), label='Test set error')\n",
    "plt.xlabel('Number of trials')\n",
    "plt.ylabel('Average Test MAE (eV)')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='both', alpha=0.1)\n",
    "\n",
    "plt.xticks([0] + x_ticks[1:], ['Untuned'] + x_ticks[1:])\n",
    "plt.xlim(-0.5, 50.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_number_of_features, optimum_number_of_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('outputs/homo_optimum_features_and_trials.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([optimum_number_of_features, optimum_number_of_trials])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
